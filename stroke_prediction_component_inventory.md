# Stroke Prediction Analysis - Essential Components Inventory

## Overview
This document provides a comprehensive inventory of all essential components required to implement the stroke prediction analysis workflow as defined in `stroke_prediction_analysis.ipynb`.

## 1. Data Files Required

### Input Dataset
- **Primary Data File**: `healthcare-dataset-stroke-data.csv`
  - **Location**: `/Users/kai/Documents/BSE/22E305_DataScience/DataScienceGang/healthcare-dataset-stroke-data.csv`
  - **Description**: Raw healthcare dataset containing stroke risk factors
  - **Required Columns**: 
    - `id`: Unique identifier
    - `gender`: Gender (Male/Female/Other)
    - `age`: Age in years
    - `hypertension`: Binary (0/1) for hypertension presence
    - `heart_disease`: Binary (0/1) for heart disease presence
    - `ever_married`: Text (Yes/No)
    - `work_type`: Categorical (Private, Self-employed, children, etc.)
    - `Residence_type`: Binary (Urban/Rural)
    - `avg_glucose_level`: Continuous glucose level
    - `bmi`: Continuous body mass index
    - `smoking_status`: Categorical (formerly smoked, never smoked, etc.)
    - `stroke`: Target variable (0/1)

### Output/Intermediate Files
- **Cleaned Dataset**: `stroke_cleaned.csv`
  - **Generated By**: Step 3 of notebook
  - **Location**: `/Users/kai/Documents/BSE/22E305_DataScience/DataScienceGang/stroke_cleaned.csv`
  - **Description**: Processed dataset after cleaning and feature engineering

## 2. Configuration Parameters

### Random Seeds & Reproducibility
```python
# Set random seed for reproducibility
np.random.seed(42)
tf.random.set_seed(42)
```

### Data Loading Parameters
```python
# Robust error handling settings
stroke_df = pl.read_csv(
    "healthcare-dataset-stroke-data.csv", 
    null_values=["N/A", ""],
    infer_schema_length=10000
)
```

### Train-Test Split Configuration
```python
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)
```

### SMOTE Configuration
```python
smote = SMOTE(sampling_strategy=0.5, random_state=42)
```

## 3. Analysis Parameters

### Feature Engineering Specifications

#### Numeric Features
- `age`: Age in years (continuous)
- `avg_glucose_level`: Glucose level (continuous)
- `bmi`: Body mass index (continuous)

#### Binary Features
- `hypertension`: Hypertension status (0/1)
- `heart_disease`: Heart disease status (0/1)
- `ever_married_binary`: Marital status (0/1)
- `residence_urban`: Urban residence (0/1)
- `gender_male`: Male gender (0/1)

#### Categorical Features
- `work_type`: Employment type (one-hot encoded)
- `smoking_status`: Smoking history (one-hot encoded)

#### Engineered Features

**Age Groups**:
```python
# Age group categories
pl.when(pl.col("age") < 18).then(pl.lit("0-17"))
.when(pl.col("age") < 40).then(pl.lit("18-39"))
.when(pl.col("age") < 60).then(pl.lit("40-59"))
.when(pl.col("age") < 80).then(pl.lit("60-79"))
.otherwise(pl.lit("80+"))
```

**BMI Categories**:
```python
# BMI classification
pl.when(pl.col("bmi") < 18.5).then(pl.lit("Underweight"))
.when(pl.col("bmi") < 25).then(pl.lit("Normal"))
.when(pl.col("bmi") < 30).then(pl.lit("Overweight"))
.otherwise(pl.lit("Obese"))
```

**Glucose Categories**:
```python
# Glucose level classification
pl.when(pl.col("avg_glucose_level") < 100).then(pl.lit("Normal"))
.when(pl.col("avg_glucose_level") < 126).then(pl.lit("Prediabetic"))
.otherwise(pl.lit("Diabetic"))
```

**Composite Risk Score**:
```python
# Risk score calculation
pl.col("hypertension") + 
pl.col("heart_disease") + 
(pl.col("age") >= 55).cast(pl.Int8) +
(pl.col("avg_glucose_level") >= 126).cast(pl.Int8) +
(pl.col("bmi") >= 30).cast(pl.Int8)
```

### Data Preprocessing Steps

#### Column Transformer Configuration
```python
preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numeric_features),
        ("bin", "passthrough", binary_features),
        ("cat", OneHotEncoder(drop="first", sparse_output=False, handle_unknown="ignore"), categorical_features),
    ]
)
```

### Model Selection Criteria

#### Random Forest Configuration
```python
RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)
```

#### Logistic Regression Configuration
```python
LogisticRegression(class_weight="balanced", max_iter=1000, random_state=42)
```

#### XGBoost Configuration
```python
XGBClassifier(n_estimators=200, max_depth=6, learning_rate=0.1, scale_pos_weight=5, random_state=42, eval_metric="logloss")
```

#### Neural Network Configuration (PyTorch)
```python
class BinaryClassifier(nn.Module):
    def __init__(self, input_dim):
        super(BinaryClassifier, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, 32)
        self.fc3 = nn.Linear(32, 16)
        self.output = nn.Linear(16, 1)
        
        self.relu = nn.ReLU()
        self.sigmoid = nn.Sigmoid()
        self.dropout1 = nn.Dropout(0.3)
        self.dropout2 = nn.Dropout(0.2)
```

#### Neural Network Training Parameters
```python
epochs = 100
patience = 10
batch_size = 32
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters())
```

### Grid Search Parameters (Elastic Net)
```python
param_grid = {
    "C": [0.001, 0.01, 0.1, 1, 10, 100],
    "l1_ratio": [0, 0.25, 0.5, 0.75, 1.0]
}

grid_search = GridSearchCV(
    elastic_net_lr,
    param_grid,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring="roc_auc",
    n_jobs=-1
)
```

### Evaluation Metrics
- **Primary Metric**: ROC-AUC (Area Under the Curve)
- **Secondary Metrics**: 
  - Accuracy
  - Precision (Stroke detection)
  - Recall (Stroke detection)
  - F1 Score
  - Classification Report
  - Confusion Matrix

## 4. Dependencies and Requirements

### Core Libraries
```python
# Data Processing
import polars as pl          # Data manipulation
import numpy as np           # Numerical computing
import pandas as pd          # Data analysis

# Visualization
import matplotlib.pyplot as plt    # Plotting
import seaborn as sns             # Statistical visualization
from plotnine import *           # Grammar of graphics (R-style)

# Machine Learning
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score, 
    roc_curve, precision_score, recall_score, f1_score, accuracy_score
)

# Imbalanced Data Handling
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline

# Gradient Boosting
from xgboost import XGBClassifier

# Deep Learning
import tensorflow as tf        # TensorFlow/Keras
from tensorflow import keras
from tensorflow.keras import layers, callbacks

# Alternative Deep Learning (PyTorch)
import torch                    # PyTorch
import torch.nn as nn          # Neural network modules
import torch.optim as optim    # Optimization
from torch.utils.data import DataLoader, TensorDataset  # Data loading
```

### Computational Requirements

#### Hardware Detection
```python
# Apple Metal (MPS) GPU Detection
if torch.backends.mps.is_available():
    device = torch.device("mps")
    print("Success: Running on Mac Metal GPU (MPS).")
else:
    device = torch.device("cpu")
    print("No MPS detected. Running on CPU.")
```

#### Memory and Processing Requirements
- **Minimum RAM**: 4GB (recommended 8GB+)
- **CPU Requirements**: Multi-core recommended (uses `n_jobs=-1`)
- **GPU Support**: Optional Apple Metal (MPS) for PyTorch acceleration
- **Storage**: ~100MB for dataset and intermediate files

### Data Pipeline Dependencies

#### Missing Value Handling
- **Method**: Median imputation for BMI values
- **Validation**: `stroke_final.null_count()` verification

#### Feature Selection Pipeline
1. **Numeric Feature Scaling**: StandardScaler
2. **Binary Feature Passthrough**: No transformation
3. **Categorical Encoding**: OneHotEncoder with `drop="first"`

#### Class Imbalance Handling
- **Method**: SMOTE (Synthetic Minority Over-sampling Technique)
- **Strategy**: `sampling_strategy=0.5`
- **Timing**: Applied only to training data (prevents data leakage)

### Validation Protocols

#### Cross-Validation Strategy
```python
StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
```

#### Train-Validation-Test Split
- **Split Ratio**: 80% train, 20% test
- **Stratification**: Maintained across all splits
- **Reproducibility**: `random_state=42` across all operations

## 5. Workflow Summary

### Analysis Pipeline Steps
1. **Data Loading**: Robust CSV loading with error handling
2. **Data Cleaning**: Missing value imputation, outlier removal
3. **Feature Engineering**: Age groups, BMI categories, glucose categories, risk score
4. **Preprocessing**: Scaling, encoding, train-test split
5. **Class Balancing**: SMOTE application to training data only
6. **Model Training**: Multiple algorithms (RF, LR, XGBoost, NN)
7. **Hyperparameter Tuning**: Grid search with cross-validation
8. **Model Evaluation**: ROC-AUC primary metric with comprehensive reporting
9. **Feature Analysis**: Coefficient analysis and feature importance

### Output Generation
- **Cleaned Dataset**: `stroke_cleaned.csv`
- **Model Performance**: Comparison table with all metrics
- **Hyperparameter Visualization**: Heatmaps for grid search results
- **Training History**: Loss and AUC plots for neural network
- **Feature Importance**: Coefficient analysis and interpretation

## 6. Reproducibility Checklist

To reproduce this analysis exactly, ensure:

- [ ] Use the exact same random seed (42) across all operations
- [ ] Maintain the same file paths and naming conventions
- [ ] Apply the same preprocessing sequence (imputation → encoding → scaling → SMOTE)
- [ ] Use identical hyperparameter ranges for grid search
- [ ] Maintain the same train-test split ratio and stratification
- [ ] Apply SMOTE only to training data, not test data
- [ ] Use the same feature engineering formulas and thresholds
- [ ] Maintain consistent library versions (especially scikit-learn, XGBoost, TensorFlow)

## 7. Performance Considerations

### Optimization Notes
- **Parallel Processing**: `n_jobs=-1` for grid search and Random Forest
- **Memory Efficiency**: Polars for large dataset handling
- **GPU Acceleration**: PyTorch with MPS support for neural network
- **Early Stopping**: Patience-based stopping in neural network training
- **Feature Selection**: L1 regularization for feature elimination

### Scalability Considerations
- **Dataset Size**: Optimized for datasets up to 100K records
- **Feature Count**: Handles high-dimensional categorical features efficiently
- **Model Complexity**: Neural network architecture scales with input dimensions
- **Cross-Validation**: 5-fold stratified CV balances accuracy and computational cost